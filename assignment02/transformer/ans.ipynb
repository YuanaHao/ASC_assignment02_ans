{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "\n",
    "## q1\n",
    "\n",
    "### BPE\n",
    "\n",
    "> start: learn from [MartinLwx's blog | https://martinlwx.github.io/zh-cn/the-bpe-tokenizer/]\n",
    "\n",
    "#### 训练流程\n",
    "\n",
    "假设有文档集$D = d_{1},d_{2},...$\n",
    "\n",
    "- 先将文档$d$变成单词列表,比如使用简单空格分词\n",
    "- 统计每个单词 $w$ 在所有文档 $D$ 中的出现频率，并计算初始`字符集 alphabet`(所有文档 $D$ 中不同的字符集合) 作为一开始的 `Vocab`(包括后面的`</w>`)\n",
    "- 先将每个单词划分为一个个 utf-8 char，称为一个划分，比如 `highest -> h, i, g, h, e, s, t`\n",
    "- 在每个单词的划分最后面加上 `</w>`，那么现在 `highest -> h, i, g, h, e, s, t, </w>`\n",
    "- 重复下面步骤直到满足两个条件中的任意一个：1）Vocab 达到上限。2）达到最大迭代次数 \n",
    "  - 找到**最经常一起出现的 pair**，并记录这个合并规则，放在 `merge table(存储合并规则的字典或哈希表)` 里面，同时把合并之后的结果放到 Vocab 里面\n",
    "  - 更新所有单词的划分，假设我们发现 `(h, i)` 最经常一起出现，那么 `hi` 就会被添加到 Vocab 里面，同时修改划分方式为：`highest -> hi, g, h, e, s, t, </w>`\n",
    "> 统计词频: 简单化找最常出现pair  \n",
    "\n",
    "> 为什么要加`</w>`: 还原输入单词完整性,标记单词边界\n",
    "\n",
    "#### 应用流程\n",
    "\n",
    "假设要处理文本`s`\n",
    "\n",
    "- 把`s`拆分成单词列表 ,每个单词拆分成uft-8 char, 单词末尾加`</w>`\n",
    "- 遍历 merge table，并检查每个合并规则是否可以用来更新每个单词的划分，可以的话就合并更新\n",
    "\n",
    "#### 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义语料库\n",
    "corpus = [\"highest\", \"higher\", \"lower\", \"lowest\", \"cooler\", \"coolest\"]\n",
    "\n",
    "# 统计词频\n",
    "count = 1\n",
    "\n",
    "# 拆分词汇并加</w>\n",
    "{\n",
    "    \"highest\": [\"h\", \"i\", \"g\", \"h\", \"e\", \"s\", \"t\", \"</w>\"],\n",
    "    \"higher\": [\"h\", \"i\", \"g\", \"h\", \"e\", \"r\", \"</w>\"],\n",
    "    \"lower\": [\"l\", \"o\", \"w\", \"e\", \"r\", \"</w>\"],\n",
    "    \"lowest\": [\"l\", \"o\", \"w\", \"e\", \"s\", \"t\", \"</w>\"],\n",
    "    \"cooler\": [\"c\", \"o\", \"o\", \"l\", \"e\", \"r\", \"</w>\"],\n",
    "    \"coolest\": [\"c\", \"o\", \"o\", \"l\", \"e\", \"s\", \"t\", \"</w>\"],\n",
    "}\n",
    "\n",
    "#合并高频pair \"es\"\n",
    "{\n",
    "    \"highest\": [\"h\", \"i\", \"g\", \"h\", \"es\", \"t\", \"</w>\"],\n",
    "    \"higher\": [\"h\", \"i\", \"g\", \"h\", \"e\", \"r\", \"</w>\"],\n",
    "    \"lower\": [\"l\", \"o\", \"w\", \"e\", \"r\", \"</w>\"],\n",
    "    \"lowest\": [\"l\", \"o\", \"w\", \"es\", \"t\", \"</w>\"],\n",
    "    \"cooler\": [\"c\", \"o\", \"o\", \"l\", \"e\", \"r\", \"</w>\"],\n",
    "    \"coolest\": [\"c\", \"o\", \"o\", \"l\", \"es\", \"t\", \"</w>\"],\n",
    "}\n",
    "\n",
    "# 合并高频pair \"est\"\n",
    "{\n",
    "    \"highest\": [\"h\", \"i\", \"g\", \"h\", \"est\", \"</w>\"],\n",
    "    \"higher\": [\"h\", \"i\", \"g\", \"h\", \"e\", \"r\", \"</w>\"],\n",
    "    \"lower\": [\"l\", \"o\", \"w\", \"e\", \"r\", \"</w>\"],\n",
    "    \"lowest\": [\"l\", \"o\", \"w\", \"est\", \"</w>\"],\n",
    "    \"cooler\": [\"c\", \"o\", \"o\", \"l\", \"e\", \"r\", \"</w>\"],\n",
    "    \"coolest\": [\"c\", \"o\", \"o\", \"l\", \"est\", \"</w>\"],\n",
    "}\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE的Huggingface实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import CharBPETokenizer\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer = CharBPETokenizer()\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    corpus, # 语料库\n",
    "    vocab_size=17, # 词汇表大小\n",
    "    min_frequency=2, # 最小词频\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 手动实现BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class BPE:\n",
    "    def __init__(\n",
    "        self,\n",
    "        corpus: list[str], # 语料库\n",
    "        vocab_size: int, # 词汇表大小\n",
    "        max_iter: int | None = None, # 最大迭代次数\n",
    "        debug: bool = False,\n",
    "    ):\n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = []\n",
    "        self.word_freq = Counter() # 词频\n",
    "        self.splits = {}  # e.g. highest: [high, est</w>] # 拆分词汇\n",
    "        self.merges = {}  # e.g. [high, est</w>]: highest # 合并词汇\n",
    "        self.max_iter = max_iter\n",
    "        self.debug = debug\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train a BPE Tokenizer\"\"\"\n",
    "        # count the word frequency\n",
    "        for document in self.corpus:\n",
    "            # split each document in corpus by whitespace\n",
    "            words = document.split()\n",
    "            self.word_freq += Counter(words)\n",
    "            # Counter 返回一个字典，字典的键是单词，值是单词出现的个数\n",
    "\n",
    "        # initialize the self.splits\n",
    "        for word in self.word_freq:\n",
    "            self.splits[word] = list(word) + [\"</w>\"]\n",
    "        # 拆分词汇并加</w>\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"Init splits: {self.splits}\")\n",
    "\n",
    "        alphabet = set() # 字符集合\n",
    "        for word in self.word_freq:\n",
    "            alphabet |= set(list(word)) # 求字符集合的并集\n",
    "        alphabet.add(\"</w>\") # 添加结束符\n",
    "\n",
    "        self.vocab = list(alphabet)\n",
    "        self.vocab.sort() # 可以不排序\n",
    "\n",
    "        cnt = 0\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            if self.max_iter and cnt >= self.max_iter: # 达到最大迭代次数\n",
    "                break\n",
    "\n",
    "            # find the most frequent pair\n",
    "            pair_freq = self.get_pairs_freq()\n",
    "\n",
    "            if len(pair_freq) == 0:\n",
    "                print(\"No pair available\")\n",
    "                break\n",
    "\n",
    "            pair = max(pair_freq, key=pair_freq.get) # 找到最高频的pair, pair其实是排序后的字符组合\n",
    "\n",
    "            self.update_splits(pair[0], pair[1]) # 更新拆分词汇\n",
    "\n",
    "            if self.debug:\n",
    "                print(f\"Updated splits: {self.splits}\")\n",
    "\n",
    "            self.merges[pair] = pair[0] + pair[1]\n",
    "\n",
    "            self.vocab.append(pair[0] + pair[1])\n",
    "\n",
    "            if self.debug:\n",
    "                print(\n",
    "                    f\"Most frequent pair({max(pair_freq.values())} times) \"\n",
    "                    f\"is : {pair[0]}, {pair[1]}. Vocab size: {len(self.vocab)}\"\n",
    "                )\n",
    "\n",
    "            cnt += 1\n",
    "    \n",
    "    def update_splits(self, lhs: str, rhs: str):\n",
    "        \"\"\"If we see lhs and rhs appear consecutively, we merge them\"\"\"\n",
    "        for word, word_split in self.splits.items():\n",
    "            new_split = []\n",
    "            cursor = 0\n",
    "            while cursor < len(word_split): # 遍历拆分词汇\n",
    "                if (\n",
    "                    word_split[cursor] == lhs # 如果当前字符是lhs\n",
    "                    and cursor + 1 < len(word_split) # 并且后面还有字符\n",
    "                    and word_split[cursor + 1] == rhs # 并且后面的字符是rhs\n",
    "                ):\n",
    "                    new_split.append(lhs + rhs) # 合并\n",
    "                    cursor += 2 # 跳过lhs和rhs\n",
    "                else:\n",
    "                    new_split.append(word_split[cursor]) # 不合并\n",
    "                    cursor += 1 # 继续遍历\n",
    "            self.splits[word] = new_split # 更新拆分词汇\n",
    "\n",
    "            # if word_split != new_split:\n",
    "            #     print(f\"old: {word_split}\")\n",
    "            #     print(f\"new: {new_split}\")\n",
    "\n",
    "    def get_pairs_freq(self) -> dict:\n",
    "        \"\"\"Compute the pair frequency\"\"\"\n",
    "        pairs_freq = defaultdict(int) # pair频率, 默认值为0\n",
    "        for word, freq in self.word_freq.items():\n",
    "            split = self.splits[word] # 拆分词汇\n",
    "            for i in range(len(split)):\n",
    "                if i + 1 < len(split):\n",
    "                    pairs_freq[(split[i], split[i + 1])] += freq # 计算pair频率, freq是词频\n",
    "\n",
    "        return pairs_freq\n",
    "\n",
    "    def tokenize(self, s: str) -> list[str]:\n",
    "        splits = [list(t) + [\"</w>\"] for t in s.split()] # 拆分字符串并加</w>\n",
    "\n",
    "        for lhs, rhs in self.merges:\n",
    "            for idx, split in enumerate(splits): # 遍历拆分词汇, idx是索引, split是拆分词汇, enumerate返回索引和元素\n",
    "                new_split = []\n",
    "                cursor = 0\n",
    "                while cursor < len(split):\n",
    "                    if (\n",
    "                        cursor + 1 < len(split)\n",
    "                        and split[cursor] == lhs\n",
    "                        and split[cursor + 1] == rhs\n",
    "                    ):\n",
    "                        new_split.append(lhs + rhs)\n",
    "                        cursor += 2\n",
    "                    else:\n",
    "                        new_split.append(split[cursor])\n",
    "                        cursor += 1\n",
    "                assert \"\".join(new_split) == \"\".join(split)\n",
    "                splits[idx] = new_split\n",
    "\n",
    "        return sum(splits, []) # 合并拆分词汇, sum函数的第二个参数是[]，表示从空列表开始累加\n",
    "\n",
    "corpus = [\"highest\", \"higher\", \"lower\", \"lowest\", \"cooler\", \"coolest\"]\n",
    "bpe = BPE(corpus, 17, debug=False)\n",
    "bpe.train()\n",
    "bpe.tokenize(\"\".join(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 局限\n",
    "\n",
    "> 把文档变成一个个单词我们这里用的是空格划分，但是像中文的话，空格并不是单词之间的边界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WorkPiece\n",
    "\n",
    "WorkPiece是BPE的一种变种, 本质上二者的训练过程相差不多, 区别在于:  \n",
    "- BPE根据pair的频率决定合并策略\n",
    "- WorkPiece是选择使得语言模型点互信息最大的相邻pair加入词表  \n",
    "\n",
    "设待合并两字词为$x$, $y$, 合并后字词为$z$, 点互信息$I(z)$有:\n",
    "$$ I(z) = I(x, y) = log\\frac{P(x, y)}{P(x)P(y)} = log\\frac{P(z)}{P(x)P(y)} $$\n",
    "\n",
    "#### 与BPE的不同\n",
    "> 在大量文本训练时, BPE的合并策略, 使得其在编码时更倾向于把单词按\"时态\"合并  \n",
    "> 而WorkPiece更倾向于按概率合并, 单词内部很可能有一个或多个有\"意义\"的单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram\n",
    "\n",
    "Unigram与前两种分词模型最大区别是, **先初始一个大词表，接着通过语言模型评估不断减少词表**，直到限定词汇量\n",
    "\n",
    "#### 训练过程\n",
    "\n",
    "> 假设: 所有subword的出现都是独立的，并且subword序列由subword出现概率的乘积产生, 就是语言模型的链式概率,即:$\\vec{x} = (x_{1},...,x_{M})$, $P(x) = \\prod_{i=1}^{M}p(x_{i})$\n",
    "  \n",
    "优化目标:  \n",
    "$$ \\Lambda = \\sum_{s=1}^{\\vert{D}\\vert}log(P(X^{(s)})) = \\sum_{s=1}^{\\vert{D}\\vert}log(\\sum P(x)) $$\n",
    "\n",
    "- 初始时构建一个相当大的字符集\n",
    "- 重复以下步骤，直到字典尺寸`|V|`减小到期望值\n",
    "  - 固定词典，通过EM算法优化$p(x)$\n",
    "  - 计算每一个子词$loss_{i}$，loss代表如果将某个词去掉，上述优化函数值会减少多少\n",
    "  - 根据$loss$排序，保留$loss$最高的n%个子词（因为要使得似然概率最大，则根据$loss$定义，应该保留$loss$大的）同时保留所有的单字符，从而避免$OOV$情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2\n",
    "\n",
    "### sentencepiece训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.train('--input=train/西游记.txt --model_prefix=train/sentencepiece/ans --vocab_size=20000 --model_type=bpe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jieba训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "# 读取.txt数据\n",
    "with open('train/西游记.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# 分词\n",
    "data_acc = jieba.cut(data, cut_all=False, HMM=True) # 精确模式分词\n",
    "\n",
    "# 保存分词结果\n",
    "df = pd.DataFrame(data_acc, columns=['word'])\n",
    "df.to_csv('train/jieba/西游记_cut.txt', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
